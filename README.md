# Systems for Machine Learning Induction Assignment 2025


## Disclaimer

This assignment may initially seem very daunting and complex. However, it is designed to be completed step-by-step, and each task is independent of one another. Remember, the goal is also to learn and grow your skills, so don't hesitate to ask for help when needed!

It will likely not be possible to complete the entire assignment in one seating. It is recommended to break it down into smaller tasks and tackle them one at a time.

Even if you are not able to complete the assignment in its entirety, please make a submission. We'd love to see how much progress you were able to make, and it is the effort that counts!

![Meme: we know this is way too steep of learning curve](assets/intro.png)


## Overview

The assignment is divided into three main tasks:
- **Task 1: Paper Reading: Attention is All You Need**
- **Task 2: Implementing FedAvg (a Federated Learning Algorithm)**
- **Task 3: Understanding about Quantization - a method to improve model performance**


Just reading these you may feel that they are a bunch of buzz-words, but don't worry! We will test you on how well you were able to navigate being thrown into the deep end of the pool :)

We will start from simple understanding of the working of modern large language models (like ChatGPT) in Task 1. 

For Task 2, we will take a detour to learn a little about Federated Learning and getting hands dirty with Pytorch. Federated Learning is a method to train machine learning models on data that is distributed across multiple devices, without the need to centralize the data. Essentially it means I can train a machine learning model without actually looking at the data. This is particularly useful for privacy-sensitive applications.

Finally in Task 3, we will learn about how to run these Large Langauge Models on small devices like your laptop or a mobile phone using Quantization.

Quantization is a technique that reduces the precision of the numbers used to represent model parameters, which can significantly decrease the model size and improve inference speed without a substantial loss in accuracy. This is particularly important for deploying models on edge devices with limited computational resources.

## Repository Structure

The repository is structured with each task contained within its own directory. The instructions and the task details are provided in the respective README.md files within each task directory.


## Instructions to Submit

Please clone this repository and complete the tasks in your copy of the repository. Once you have completed the tasks, you can push the changes to a **private** GitHub repository and add the following GitHub users as [collaborators](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/repository-access-and-collaboration/inviting-collaborators-to-a-personal-repository) to your repository:
- vimarsh244
- pranav-m-r
- sohlost


## Any Questions?

Please feel free to reach out to any of us over Slack or email if you have any questions or need any clarifications regarding the assignment.

- Vimarsh Shah
- Atharva Pandit
- Pranav M R
- Soham Dambalkar


Again, it is completely fine if you are not able to complete the entire assignment. Please do make a submission with whatever you were able to complete. Good luck and happy learning!
